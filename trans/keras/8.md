# 附录

代码清单 A-1 中展示的程序在第 4 章中用于将克里夫兰心脏病数据集文件拆分为训练、验证和测试文件。

代码清单 A-1

```
  #
  split_file.py
  #
  does not read source into memory
  #
  useful when no processing/normalization needed

  import numpy as np

  def file_len(fname):
   f
  = open(fname)
   for (i, line) in enumerate(f): pass
   f.close()
   return i+1

  def main():

  source_file = ".\\cleveland_norm.txt"

  train_file = ".\\cleveland_train.txt"

  validate_file = ".\\cleveland_validate.txt"

  test_file = ".\\cleveland_test.txt"

  N = file_len(source_file)
    num_train
  = int(0.60 * N)

  num_validate = int(0.20 * N)

  num_test = N - (num_train + num_validate) # ~20%

    np.random.seed(1)

  indices = np.arange(N)  # array [0, 1, . . N-1]
    np.random.shuffle(indices)

  train_dict = {}

  test_dict = {}

  validate_dict = {}
    for i in range(0,num_train):

  k = indices[i]; v = i  #
  i is not used

  train_dict[k] = v

    for i in range(num_train,(num_train+num_validate)):

  k = indices[i]; v = i

  validate_dict[k] = v  

    for i in range((num_train+num_validate),N):

  k = indices[i]; v = i

  test_dict[k] = v 

  f_source = open(source_file, "r")

  f_train = open(train_file, "w")

  f_validate = open(validate_file, "w")

  f_test = open(test_file, "w")

  line_num = 0
    for line in
  f_source:

  if line_num in train_dict: # checks for key

  f_train.write(line)

  elif line_num in validate_dict:

  f_validate.write(line)

  else:

  f_test.write(line)

  line_num += 1

  f_source.close()

  f_train.close()

  f_validate.close()

  f_test.close() 

  if __name__ == "__main__":

  main()

```

代码清单 A-2 中的程序在第 5 章中被用来创建 MNIST 培训和测试文件。

代码清单 A-2

```
  #
  make_data.py
  #
  raw binary MNIST to Keras text file
  #
  #
  go to http://yann.lecun.com/exdb/mnist/ and
  #
  download the four g-zipped files:
  #
  train-images-idx3-ubyte.gz (60,000 train images) 
  #
  train-labels-idx1-ubyte.gz (60,000 train labels) 
  #
  t10k-images-idx3-ubyte.gz  (10,000 test images) 
  #
  t10k-labels-idx1-ubyte.gz  (10,000 test labels) 
  # 
  #
  use the 7-Zip program to unzip the four files.
  #
  I recommend adding a .bin extension to remind
  #
  you they're in a proprietary binary format
  #
  #
  run the script twice, once for train data, once for
  #
  test data, changing the file names as appropriate.
  #
  uses pure Python only

  #
  target format:
  #
  5 ** 0 0 152 27 .. 0
  #
  7 ** 0 0 38 122 .. 0
  #
  label digit at [0]    784 vals at [2-786]
  #
  dummy ** seperator at [1] 

  def generate(img_bin_file, lbl_bin_file,

  result_file, n_images):

  img_bf = open(img_bin_file, "rb")    #
  binary image pixels

  lbl_bf = open(lbl_bin_file, "rb")    #
  binary labels

  res_tf = open(result_file, "w")      #
  result file

  img_bf.read(16)   #
  discard image header info

  lbl_bf.read(8)    #
  discard label header info

    for i in range(n_images):   # number images requested 

  # digit label first

  lbl = ord(lbl_bf.read(1))  # get label like '3' (one byte)

  res_tf.write(str(lbl))

  # encoded = [0] *
  10         # make one-hot vector

  # encoded[lbl] = 1
      # for i in range(10):

  # 
  res_tf.write(str(encoded[i]))

  #  res_tf.write("
  ")  # like 0 0 0 1 0 0 0 0 0 0 

  res_tf.write("
  ** ")  # arbitrary seperator char for
  readibility

  # now do the image
  pixels

  for j in
  range(784):  # get 784 vals for each image file

  val = ord(img_bf.read(1))

  res_tf.write(str(val))

  if j != 783: res_tf.write(" ")  # avoid trailing space 

  res_tf.write("\n")  # next image

  img_bf.close(); lbl_bf.close();  # close the binary files

  res_tf.close()                   # close the result text file

  #
  ================================================================

  def main():
    #
  generate(".\\UnzippedBinary\\train-images.idx3-ubyte.bin",
    #        
  ".\\UnzippedBinary\\train-labels.idx1-ubyte.bin",
    #        
  ".\\mnist_train_keras_1000.txt",
    #         n_images = 1000)  # first
  n images

  generate(".\\UnzippedBinary\\t10k-images.idx3-ubyte.bin",

  ".\\UnzippedBinary\\t10k-labels.idx1-ubyte.bin",

  ".\\mnist_test_keras_foo.txt",

         n_images = 100)  #
  first n images

  if __name__ == "__main__":

  main()

```

代码清单 A-3 中显示的程序在第 5 章中用于显示 MNIST 数字。

代码清单 a3

```
  #
  show_image.py

  import numpy as np
  import matplotlib.pyplot as
  plt

  #
  data file looks like:
  #
  5 ** 0 .. 23 157 .. 0
  #
  4 ** 0 .. 255 16 .. 0
  #
  note dummy separator at [1]

  def display(txt_file, idx):
    # values between 0-255
    # data file has 1 + 1 + 784 = 786
  vals per line, [0] to [785]

  y_data = np.loadtxt(txt_file, delimiter = " ",

  usecols=[0], dtype=np.float32)

  x_data = np.loadtxt(txt_file, delimiter = " ",

  usecols=range(2,786), dtype=np.float32)

  label = int(y_data[idx])  # like '5'

  print("digit =
  ", str(label), "\n")

  pixels = np.array(x_data[idx,], dtype=np.int)  #
  to int

  pixels = pixels.reshape((28,28))
    for i in range(28):

  for j in
  range(28):

  print("%.2X" % pixels[i,j], end="")

  print(" ", end="")

  print("")

  img = np.array(x_data[idx,])   # as float32

  img = img.reshape((28,28))
    plt.imshow(img, cmap=plt.get_cmap('gray_r'))
    plt.show()  

  def main():

  print("\nBegin
  show MNIST image demo \n")

  img_file = ".\\mnist_train_keras_1000.txt"

  display(img_file, idx=0)  # first image

  print("\nEnd
  \n")

  if __name__ == "__main__":

  main()

```

代码清单 A-4 中展示的程序在第 6 章中用于创建 IMDB 电影回顾培训和测试文件。

代码清单 A-4

```
  #
  make_data_files.py
  #
  #
  input: source Stanford 50,000 data files reviews
  #
  output: one combined train file, one combined test file
  #
  output files are in index version, using the Keras dataset
  #
  format where 0 = padding, 1 = 'start', 2 = OOV, 3 = unused
  #
  4 = most frequent word ('the'), 5 = next most frequent, etc.
  #
  i'm skipping the start=1 because it makes no sense.
  #
  these data files will be loaded into memory then feed
  #
  a built-in Embedding layer (rather than custom embeddings)

  import os

  #
  allow the Windws cmd shell to deal with wacky characters
  import sys
  import codecs
  sys.stdout = codecs.getwriter('utf8')(sys.stdout.buffer)

  #
  ---------------------------------------------------------------

  def get_reviews(dir_path, num_reviews, punc_str):

  punc_table = {ord(char): None
  for char
  in punc_str}  # dictionary

  reviews = []  #
  list-of-lists of words

  ctr = 1
    for file in os.listdir(dir_path):

  if ctr > num_reviews: break

  curr_file = os.path.join(dir_path, file)

  f = open(curr_file, "r", encoding="utf8")   # each review has only one line . . . 

  for line in
  f:

  line = line.strip()

  if len(line) > 0:  # number characters

  # print(line)  # to
  show non-ASCII == errors

  line = line.translate(punc_table)  # remove punc

  line = line.lower()  #
  lower case

  line = " ".join(line.split())  # remove consecutive WS

  word_list = line.split("
  ")  # one review is a list of words

  reviews.append(word_list)    # 

  f.close()  # close curr
  file

  ctr += 1
    return reviews

  #
  ---------------------------------------------------------------

  def make_vocab(all_reviews):

  word_freq_dict = {}   #
  key = word, value = frequency

    for i in range(len(all_reviews)):

  reviews = all_reviews[i]

  for review in reviews:

  for word in
  review:

  if word in
  word_freq_dict:

  word_freq_dict[word] += 1

  else:

  word_freq_dict[word] = 1

  kv_list = []  # list of
  word-freq tuples so can sort
    for (k,v) in
  word_freq_dict.items():

  kv_list.append((k,v))

    # list of tuples where index is
  0-based rank, val is (word,freq)

  sorted_kv_list = sorted(kv_list, key=lambda x: x[1], reverse=True)  #
  sort by freq

  f = open(".\\vocab_file.txt", "w", encoding="utf8")

  vocab_dict = {}  # key
  = word, value = 1-based rank ('the' = 1, 'a' = 2, etc.)
    for i in range(len(sorted_kv_list)):

  w = sorted_kv_list[i][0]  # word is at [0]

  vocab_dict[w] = i+1       # 1-based as in Keras dataset

  f.write(w + "
  " + str(i+1) + "\n")  # save
  word-space-index

  f.close()

    return vocab_dict

  #
  ---------------------------------------------------------------

  def generate_file(reviews_lists, outpt_file, w_or_a, vocab_dict, max_review_len, label_char):

  fout = open(outpt_file, w_or_a, encoding="utf8")  # write first time, append later

  offset = 3  # Keras
  offset: 'the' = 1 (most frequent) -> 1+3 = 4

    for i in range(len(reviews_lists)):  # walk thru each review-list

  curr_review = reviews_lists[i]

  n_words = len(curr_review)     

  if n_words > max_review_len:

  continue  # next i, continue without writing anything

  n_pad = max_review_len - n_words   # number of 0s to prepend

  for j in
  range(n_pad):

  fout.write("0
  ")

  for word in
  curr_review:      

  if word not
  in vocab_dict:  # a
  word in test set not in training set

  fout.write("2
  ")   # 2 is the special out-of-vocab
  index        

  else:

  idx = vocab_dict[word] + offset

  fout.write("%d
  " % idx)

  fout.write(label_char + "\n")  # like
  '0' or '1', or 'N' or 'P"

  fout.close()

  #
  ---------------------------------------------------------------          

  def main():

  remove_chars = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~"   # leave ' for words like it's

  print("\nLoading
  all reviews into memory - be patient ")

  pos_train_reviews = get_reviews(".\\SourceFiles\\train\\pos", 12500, remove_chars)

  neg_train_reviews = get_reviews(".\\SourceFiles\\train\\neg", 12500, remove_chars)

  pos_test_reviews = get_reviews(".\\SourceFiles\\test\\pos", 12500, remove_chars)

  neg_test_reviews = get_reviews(".\\SourceFiles\\test\\neg", 12500, remove_chars)

  mp = max(len(l) for
  l in
  pos_train_reviews)  #
  2469

  mn = max(len(l) for
  l in
  neg_train_reviews)  #
  1520

  mm = max(mp, mn)  #
  longest (in words) review in training set  # 2469
    # print(mp, mn)

    print("\nAnalyzing reviews and
  making vocabulary ")

  vocab_dict = make_vocab([pos_train_reviews, neg_train_reviews])  # key = word, value = word rank

  v_len = len(vocab_dict)  #
  need this value, plus 4, for Embedding Layer: 129888+4 = 129892

  print("\nVocab
  size = %d -- use this +4 for Embedding nw " % v_len)

  max_review_len = 50   #
  use None for all reviews (any len)
    if max_review_len == None
  or max_review_len > mm:

  max_review_len = mm

  print("\nGenerating
  training file with len %d words or less " % max_review_len)

  generate_file(pos_train_reviews, ".\\imdb_train_50w.txt", "w", vocab_dict, max_review_len, "1")

  generate_file(neg_train_reviews, ".\\imdb_train_50w.txt", "a", vocab_dict, max_review_len, "0")

  print("\nGenerating
  test file with len %d words or less " % max_review_len)

  generate_file(pos_test_reviews, ".\\imdb_test_50w.txt", "w", vocab_dict, max_review_len, "1")

  generate_file(neg_test_reviews, ".\\imdb_test_50w.txt", "a", vocab_dict, max_review_len, "0")

    # inspect a generated file
    # vocab_dict was used indirectly
  (offset)

    # print("Displaying encoded
  training file: \n")
    # f =
  open(".\\imdb_train_50w.txt", "r",
  encoding="utf8")
    # for line in f: 
    #   print(line, end="")
    # f.close()

    # print("\nDisplaying decoded
  training file: \n") 

    # index_to_word = {}
    # index_to_word[0] =
  "<PAD>"
    # index_to_word[1] =
  "<ST>"
    # index_to_word[2] =
  "<OOV>"
    # for (k,v) in vocab_dict.items():
    #   index_to_word[v+3] = k

    # f =
  open(".\\imdb_train_50w.txt", "r", encoding="utf8")
    # for line in f:
    #   line = line.strip()
    #   indexes = line.split("
  ")
    #   for i in range(len(indexes)-1): 
  # last is '0' or '1'
    #     idx = (int)(indexes[i])
    #     w = index_to_word[idx]
    #     print("%s " % w,
  end="")
    #   print("%s \n" %
  indexes[len(indexes)-1])
    # f.close()

  if __name__ == "__main__":

  main()

```

代码清单 A-5 中的程序在第 7 章中用于显示 UCI 数字数据集图像。

代码清单 a5:程序显示

```
  #
  show_digit.py

  import numpy as np
  import matplotlib.pyplot as
  plt

  #
  data file looks like:
  #
  0,0,5,16 . . 12,0,0,7
  #
  first 64 values are grayscale pixel (0-16), last is digit (0-9)

  def display(data_file, idx):

  x_data = np.loadtxt(data_file, delimiter = ",",

  usecols=range(0,64), dtype=np.int)

  y_data = np.loadtxt(data_file, delimiter = ",",

  usecols=[64], dtype=np.int)

  label = y_data[idx]  # like '5'

  print("digit =
  ", str(label), "\n")

  pixels = np.array(x_data[idx])  # target row of pixels

  pixels = pixels.reshape((8,8))
    for i in range(8):

  for j in
  range(8):

  print("%.2X" % pixels[i,j], end="")

  print(" ", end="")

  print("")

    plt.imshow(pixels, cmap=plt.get_cmap('gray_r'))
    plt.show()  

  def main():

  print("\nBegin
  show UCI mini-digit \n")

  data_file = ".\\digits_uci_test_1797.txt"

  display(data_file, idx=8) 

  print("\nEnd
  \n")

  if __name__ == "__main__":

  main()

```