# 结论

最后，我要引用斯图亚特·拉塞尔和彼得·诺维格的话，他们写道:

”*你可以说支持向量机的成功是因为一个关键的洞察力，一个巧妙的技巧。*”

(Russell & Norvig，2010)

关键的见解是一些例子比其他例子更重要。它们最接近决策边界，我们称之为**支持向量**。结果，我们发现最优超平面比其他超平面具有更好的泛化能力，并且可以仅使用支持向量来构造。我们详细看到，我们需要解决一个凸优化问题来找到这个超平面。

巧妙的技巧是**核心技巧**。它允许我们对不可分离的数据使用支持向量机，没有它，支持向量机将非常有限。我们看到这个技巧，虽然一开始可能很难掌握，但实际上相当简单，可以在其他学习算法中重用。

就这样。如果你把这本书从头到尾看了一遍，你现在应该明白*和* SVMs 是如何工作的了。另一个有趣的问题是*他们为什么工作？这是一个名为计算学习理论的领域的主题(支持向量机实际上来自统计学习理论)。如果你想了解更多这方面的知识，你可以关注这个[优秀课程](http://work.caltech.edu/telecourse.html)或者阅读*从数据中学习* (Abu-Mostafa，2012)，它提供了一个关于这个主题的非常好的介绍。*

您应该知道，支持向量机不仅仅用于分类。一类 SVM 可以用于异常检测，支持向量回归可以用于回归。为了保持简洁，本书没有包含它们，但它们是同样有趣的话题。既然你理解了基本的支持向量机，你应该更好地准备学习这些推导。

支持向量机不会解决你所有的问题，但我真的希望它们现在会成为你机器学习工具箱中的一个工具——一个你理解的工具，你会喜欢使用的工具。