# 第五章软缘 SVM

## 处理有噪声的数据

**硬边界 SVM** 最大的问题是它要求数据是线性可分的。现实生活中的数据经常是嘈杂的。即使当数据是线性可分的，在你把它输入到你的模型之前，很多事情都会发生。例如，可能有人输入了错误的值，或者传感器的探头返回了一个疯狂的值。在存在异常值(一个看起来不在其组中的数据点)的情况下，有两种情况:异常值可以比其类的大多数示例更接近其他示例，从而减少边距，或者它可以在其他示例中并打破线性可分性。让我们考虑一下这两个案例，看看 SVM 是如何处理的。

### 降低边际的异常值

当数据是线性可分的时，硬边界分类器在异常值存在的情况下不会像我们希望的那样工作。

现在让我们考虑我们的数据集，在(5，7)增加一个异常数据点，如图 33 所示。

![](../Images/image380.png)

图 33:数据集仍然可以与(5，7)处的异常值线性分离

在这种情况下，我们可以看到边际非常窄，似乎异常值是这种变化的主要原因。直观地说，我们可以看到，这个超平面可能不是分离数据的最佳选择，并且它可能很难推广。

### 离群打破线性可分性

更糟糕的是，当离群点打破线性可分性时，如图 34 中的点(7，8)所示，分类器无法找到超平面。我们因为一个数据点而陷入困境。

![](../Images/image381.png)

图 34 :( 7，8)处的异常值打破了线性可分性

## 软缘来救援

### 松弛变量

1995 年，瓦普尼克和科尔特斯引入了原始 SVM 的修改版本，允许分类器犯一些错误。现在的目标不是犯零分类错误，而是犯尽可能少的错误。

为此，他们通过添加变量![](../Images/image382.png)(ζ)来修改优化问题的约束。所以约束条件是:

![](../Images/image383.png)

变成:

![](../Images/image384.png)

结果，当最小化目标函数时，即使示例不满足原始约束(即，它离超平面太近，或者它不在超平面的正确侧)，也有可能满足约束。代码清单 29 对此进行了说明。

代码清单 29

```
import numpy as np

w = np.array([0.4, 1])
b = -10

  x = np.array([6, 8])
y = -1

  def  constraint(w, b, x, y):
    return  y * (np.dot(w, x) + b)
  def  hard_constraint_is_satisfied(w, b, x, y):
    return  constraint(w, b, x, y) >= 1

  def  soft_constraint_is_satisfied(w, b, x, y, zeta):
    return  constraint(w, b, x, y) >= 1 - zeta

  # While the constraint is not satisfied for the example (6,8).
  print (hard_constraint_is_satisfied(w, b, x, y))               # False

# We can use zeta = 2 and satisfy the soft constraint.
  print (soft_constraint_is_satisfied(w, b, x, y, zeta=2))       # True

```

问题是我们可以为每个例子选择一个巨大的![](../Images/image382.png)值，所有的约束都会得到满足。

代码清单 30

```
# We can pick a huge zeta for every point
# to always satisfy the soft constraint.
  print (soft_constraint_is_satisfied(w, b, x, y, zeta=10))   # True
  print (soft_constraint_is_satisfied(w, b, x, y, zeta=1000)) # True

```

为了避免这种情况，我们需要修改目标函数来惩罚大![](../Images/image385.png)的选择:

![](../Images/image386.png)

我们取所有个体的总和![](../Images/image385.png)并将其加入目标函数。增加这样的处罚叫做**正规化**。结果，解决方案将是最大化余量同时具有最小可能误差的超平面。

还是有点问题。有了这个公式，人们可以很容易地通过使用![](../Images/image385.png)的负值来最小化函数。我们添加了约束![](../Images/image387.png)来防止这种情况。此外，我们希望对软保证金保持一定的控制。也许有时我们想使用硬边界——毕竟，这就是为什么我们添加参数![](../Images/image388.png)，这将帮助我们确定![](../Images/image382.png) 应该有多重要(稍后将详细说明)。

这就引出了**软余量公式**:

![](../Images/image389.png)

如(Vapnik V. N .，1998)所示，使用与可分离情况相同的技术，我们发现我们需要在稍微不同的约束下最大化**与之前相同的 Wolfe 对偶:**

![](../Images/image390.png)

这里的约束![](../Images/image391.png)已经变成了![](../Images/image392.png)。该约束通常被称为**框约束**，因为向量![](../Images/image393.png)被约束为位于框内，边长![](../Images/image388.png)为正或负。请注意，一个正方是平面上一个象限的模拟 n 维欧氏空间(cristiani&Shawe-Taylor，2000)。在关于 SMO 算法的章节中，我们将可视化图 50 中的框约束。

优化问题也被称为**1-范数软裕度**，因为我们正在最小化松弛向量的 1-范数![](../Images/image382.png)。

## 理解 C 做什么

参数![](../Images/image388.png)让你控制 SVM 如何处理错误。现在让我们来研究改变它的值将如何给出不同的超平面。

图 35 显示了我们在本书中使用的线性可分数据集。在左侧，我们可以看到将![](../Images/image388.png)设置为![](../Images/image394.png)给出了与硬边界分类器相同的结果。然而，如果我们像在中心一样为![](../Images/image388.png)选择一个较小的值，我们可以看到超平面比其他点更接近一些点。这些示例违反了硬边界约束。设置![](../Images/image395.png)会增加该行为，如右图所示。

如果我们选择一个非常接近零的![](../Images/image388.png)会发生什么？那么基本上就不再有约束了，我们最终得到了一个不分类任何东西的超平面。

![](../Images/image396.png)

图 35:C =+无穷大、C=1 和 C=0.01 对线性可分数据集的影响

似乎在数据线性可分的情况下，坚持大![](../Images/image388.png)是最好的选择。但是如果我们有一些嘈杂的异常值呢？在这种情况下，如图 36 所示，使用![](../Images/image397.png)给我们一个非常窄的余量。然而，当我们使用![](../Images/image398.png)时，我们得到的超平面非常接近没有离群点的硬边界分类器的超平面。唯一违反的约束是离群点的约束，我们对这个超平面更加满意。这一次，设置![](../Images/image395.png)最终违反了另一个例子的约束，这不是一个异常值。![](../Images/image388.png)的这个值似乎给了我们的软边界分类器太多的自由。

![](../Images/image399.png)

图 36:C =+无穷大、C=1 和 C=0.01 对具有异常值的线性可分数据集的影响

最终，在异常值使数据不可分离的情况下，我们不能使用![](../Images/image397.png)，因为没有满足所有硬边界约束的解决方案。相反，我们测试![](../Images/image388.png)的几个值，我们看到最佳超平面是通过![](../Images/image400.png)实现的。事实上，对于![](../Images/image388.png)大于或等于 3 的所有值，我们得到了相同的超平面。这是因为无论我们如何惩罚它，都有必要违反离群值的约束来分离数据。当我们像以前一样使用小![](../Images/image388.png)时，更多的约束被违反。

![](../Images/image401.png)

图 37:C = 3、C=1 和 C=0.01 对具有异常值的不可分离数据集的影响

经验法则:

*   一个小的![](../Images/image388.png)将给出更大的裕度，代价是一些错误的分类。
*   一个巨大的![](../Images/image388.png)将给出硬边界分类器并容忍零约束违反。
*   关键是要找到![](../Images/image388.png)的值，使得有噪声的数据不会对解造成太大影响。

## 如何找到最好的 C？

![](../Images/image388.png)没有什么神奇的价值可以解决所有的问题。选择![](../Images/image388.png)的推荐方法是使用[网格搜索](http://scikit-learn.org/stable/modules/grid_search.html)和[交叉验证](http://scikit-learn.org/stable/modules/cross_validation.html)(徐，常，&林，*支持向量分类实用指南*)。要理解的关键是![](../Images/image388.png)的值对你正在使用的数据非常具体，所以如果有一天你发现 C=0.001 对你的一个问题不起作用，你还是应该用另一个问题来尝试这个值，因为它不会有同样的效果。

## 其他软边界公式

### 2-定额软余量

这个问题还有另一种表述，叫做**2-范数(或 L2 正则化)软裕度**，我们在其中最小化![](../Images/image402.png)。这个公式导致了一个没有盒子约束的沃尔夫对偶问题。有关 2 范数软边界的更多信息，请参考(克里斯蒂亚诺尼&肖-泰勒，2000)。

### 怒-SVM

因为![](../Images/image388.png)的尺度受特征空间的影响，所以提出了问题的另一种表述方式:![](../Images/image403.png)T2。其思想是使用一个值在 0 和 1 之间变化的参数![](../Images/image405.png)，而不是参数![](../Images/image388.png)。

| ![](../Images/note.png) | 注:“![](../Images/image407.png)给出了问题更透明的参数化，不依赖于特征空间的缩放，只依赖于数据中的噪声水平。”(克里斯蒂亚诺尼&肖-泰勒，2000) |

要解决的优化问题是:

![](../Images/image408.png)

## 总结

软边缘 SVM 公式是对硬边缘分类器的一个很好的改进。它允许我们对数据进行正确分类，即使存在破坏线性可分性的噪声数据。然而，这种额外灵活性的代价是我们现在有了一个超参数![](../Images/image388.png)，我们需要为它找到一个值。我们看到了改变![](../Images/image388.png)的值如何影响裕度，并允许分类器犯一些错误以获得更大的裕度。这再次提醒我们，我们的目标是找到一个假设，它将在看不见的数据上很好地工作。如果模型最终能很好地推广，那么训练数据上的一些错误并不是一件坏事。